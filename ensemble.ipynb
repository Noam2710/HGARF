{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "from deap import base, creator, tools, algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import timeit\n",
    "import copy\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from scipy import stats as st\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import warnings\n",
    "import multiprocessing\n",
    "from hyperopt import fmin, tpe, hp, Trials\n",
    "import smtplib, ssl\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Global definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"../input/150-classification/classification_datasets/\"\n",
    "results_path = \"../input/results/result_df.csv\"\n",
    "ada_path = \"../input/adaours/ada-ours.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Side Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.read_csv(ada_path)\n",
    "# # df_results.head()\n",
    "# df_results[\"diff\"] = df_results.apply(lambda x: x[\"HGARF_test_score\"] - x[\"ada_test_score\"], axis=1)\n",
    "# df_results[\"Status\"] = df_results[\"diff\"].apply(lambda x: \"Win\" if x > 0 else \"Loss\" if x < 0 else \"Tie\")\n",
    "# df_results[df_results.dataset=='cardiotocography-10clases']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Done side checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_decision_trees = None\n",
    "population_with_predictions = None\n",
    "# pool = multiprocessing.Pool()\n",
    "\n",
    "\n",
    "\n",
    "def register_stats():\n",
    "    global stats\n",
    "    stats.register(\"avg\", np.mean)\n",
    "    stats.register(\"std\", np.std)\n",
    "    stats.register(\"min\", np.min)\n",
    "    stats.register(\"max\", np.max)\n",
    "\n",
    "\n",
    "def create_forest(total_number_of_trees, num_of_features):\n",
    "    global population_with_predictions\n",
    "    \n",
    "    RF_PREDICTIONS_X_VAL = []\n",
    "    RF = []\n",
    "    \n",
    "    \n",
    "    X, y = x_training, y_training\n",
    "    if num_of_features <= 2:\n",
    "        curr_tree = RandomForestClassifier(n_estimators=int(total_number_of_trees),\n",
    "                                           max_features=num_of_features)\n",
    "        curr_tree.fit(X, y)\n",
    "        RF.append(curr_tree)\n",
    "    else:\n",
    "        for i in range(2, num_of_features):\n",
    "            curr_tree = RandomForestClassifier(n_estimators=int(total_number_of_trees / (num_of_features - 2)),\n",
    "                                               max_features=i)\n",
    "            curr_tree.fit(X, y)\n",
    "            RF.append(curr_tree)\n",
    "\n",
    "    # Flatten the list to one list of N decition trees\n",
    "    RF = [item for sublist in RF for item in sublist]\n",
    "    \n",
    "    for tree in RF:\n",
    "        RF_PREDICTIONS_X_VAL.append(tree.predict(x_val))\n",
    "        \n",
    "    population_with_predictions = np.array(RF_PREDICTIONS_X_VAL)\n",
    "    \n",
    "    return RF\n",
    "\n",
    "def create_population(list_of_decision_trees, pop_size, chromosome_length):\n",
    "    population = []\n",
    "    for _ in range(pop_size):\n",
    "        individual = random.sample(list_of_decision_trees, chromosome_length)\n",
    "        population.append(individual)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cx_one_point(ind1, ind2):\n",
    "    cxpoint = random.randint(0, len(ind1))\n",
    "\n",
    "    old_ind2 = ind2[cxpoint:]\n",
    "    old_ind1 = ind1[cxpoint:]\n",
    "\n",
    "    intersection = set(ind1[cxpoint:]).intersection(set(old_ind2))\n",
    "    while len(intersection) > 0:\n",
    "        for intersect in intersection:\n",
    "            old_ind2.remove(intersect)\n",
    "            old_ind2.append(random.randint(0,len(list_of_decision_trees)-1))\n",
    "        intersection = set(ind1[cxpoint:]).intersection(set(old_ind2))\n",
    "\n",
    "    intersection = set(ind2[cxpoint:]).intersection(set(old_ind1))\n",
    "    while len(intersection) > 0:\n",
    "        for intersect in intersection:\n",
    "            old_ind1.remove(intersect)\n",
    "            old_ind1.append(random.randint(0, len(list_of_decision_trees)-1))\n",
    "        intersection = set(ind2[cxpoint:]).intersection(set(old_ind1))\n",
    "\n",
    "    ind1[cxpoint:], ind2[cxpoint:] = old_ind2, old_ind1\n",
    "    return ind1, ind2\n",
    "\n",
    "def initialize_evolution_functions(total_number_of_trees, chromosome_length, tour_size):\n",
    "\n",
    "    global toolbox\n",
    "\n",
    "#     toolbox.register(\"map\", pool.map)\n",
    "    \n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "    toolbox.register(\"attribute\", random.randint, 0, total_number_of_trees-1)\n",
    "    toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attribute, chromosome_length)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    toolbox.register(\"evaluate\", evaluate_individual)\n",
    "    toolbox.register(\"mate\", cx_one_point)\n",
    "    toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=total_number_of_trees-1, indpb=0.2)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize=tour_size)\n",
    "    register_stats()\n",
    "\n",
    "    pop = toolbox.population(n=200)\n",
    "    fitnesses = toolbox.map(toolbox.evaluate, pop)\n",
    "    best_ind = tools.HallOfFame(1)\n",
    "    pop = update_fitnesses_to_population(pop,fitnesses)\n",
    "\n",
    "    return pop, best_ind\n",
    "\n",
    "def evaluate_individual(individual, test=False):\n",
    "\n",
    "    if test:\n",
    "        X, y = x_test, y_test\n",
    "    else:\n",
    "        X, y = x_val, y_val\n",
    "\n",
    "    prediction_results = []\n",
    "    for decision_tree_index in individual:\n",
    "        if test:\n",
    "            prediction_results.append(list_of_decision_trees[decision_tree_index].predict(X))\n",
    "        else:\n",
    "            prediction_results.append(population_with_predictions[decision_tree_index])\n",
    "\n",
    "    prediction_results = np.array(prediction_results)\n",
    "    predictions_of_individual = st.mode(prediction_results)[0].squeeze()\n",
    "    fitness = accuracy_score(predictions_of_individual, y)\n",
    "\n",
    "    return (float(fitness),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_population_to_deap(pop_not_for_deap):\n",
    "    pop_to_deap = []\n",
    "    for ind in pop_not_for_deap:\n",
    "        pop_to_deap.append(creator.Individual(ind))\n",
    "\n",
    "    return pop_to_deap\n",
    "\n",
    "def update_fitnesses_to_population(pop, fitnesses):\n",
    "    for ind, fit in zip(pop, fitnesses):\n",
    "        ind.fitness.values = fit\n",
    "\n",
    "    return pop\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "def label_encode(df):\n",
    "    le = LabelEncoder()\n",
    "    target_col = df.columns[-1]\n",
    "    le.fit(df[target_col])\n",
    "    df[target_col] = le.transform(df[target_col])\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_df_to_x_y(df):\n",
    "    target_col = df.columns[-1]\n",
    "    y = df[target_col].to_numpy()\n",
    "    del df[target_col]\n",
    "    X = df.values\n",
    "    return X,y\n",
    "\n",
    "\n",
    "def train_val_test_split(X,y, test_size, val_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=1)\n",
    "    return X_train, X_val, y_train, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "def handle_categorial(df):\n",
    "    new_df = copy.deepcopy(df)\n",
    "    for item_idx, (key, val) in enumerate(df.dtypes.items()):\n",
    "        if item_idx == df.dtypes.size - 1:\n",
    "            break\n",
    "        if val == object:\n",
    "            dummy = pd.get_dummies(df[key], dtype=float)\n",
    "            new_df = pd.concat([dummy, new_df], axis=1)\n",
    "            del new_df[key]\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def calculate_legal_trees_number(num_of_features, trees=1000):\n",
    "    if num_of_features <= 2:\n",
    "        return trees\n",
    "    while (True):            \n",
    "        if trees % (num_of_features - 2) == 0:\n",
    "            return trees\n",
    "        else:\n",
    "            trees -= 1\n",
    "            \n",
    "def get_common_val(df,col):\n",
    "    for key, val in df[col].value_counts().items():\n",
    "        return key\n",
    "            \n",
    "def fill_na(df):\n",
    "    for item_idx, (key, val) in enumerate(df.dtypes.items()):\n",
    "        if item_idx == df.dtypes.size - 1:\n",
    "            break\n",
    "        if val == object:\n",
    "            most_common = get_common_val(df,key)\n",
    "            df[key].fillna(most_common, inplace=True)\n",
    "        if val == float or val == int:\n",
    "            med = df[key].median()\n",
    "            df[key].fillna(med, inplace=True)\n",
    "    return df\n",
    "\n",
    "def preprocess(df):\n",
    "    df = fill_na(df)\n",
    "    df = handle_categorial(df)\n",
    "    df = label_encode(df)\n",
    "    return df\n",
    "\n",
    "def initialize_data_set(curr_dataset_path, test_size, val_size):\n",
    "    df = pd.read_csv(curr_dataset_path)\n",
    "    df = preprocess(df)\n",
    "    X, y = convert_df_to_x_y(df)\n",
    "    num_of_features = df.shape[1]\n",
    "    X_train, X_val, y_train, y_val, X_test, y_test = train_val_test_split(X,y, test_size, val_size)\n",
    "    return X_train, X_val, y_train, y_val, X_test, y_test, num_of_features\n",
    "\n",
    "def initialize_x_y(curr_dataset_path):\n",
    "    df = pd.read_csv(curr_dataset_path)\n",
    "    df = preprocess(df)\n",
    "    X, y = convert_df_to_x_y(df)\n",
    "    num_of_features = df.shape[1]\n",
    "    return X,y, num_of_features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HGARF(total_number_of_trees, num_of_features, chromosome_length, number_of_generations, tour_size, crossover_rate,mutation_rate):\n",
    "\n",
    "    global list_of_decision_trees\n",
    "    \n",
    "    total_number_of_trees = calculate_legal_trees_number(num_of_features)\n",
    "    \n",
    "    list_of_decision_trees = create_forest(total_number_of_trees=total_number_of_trees, num_of_features=num_of_features)\n",
    "\n",
    "    pop, best_ind = initialize_evolution_functions(total_number_of_trees, chromosome_length, tour_size)\n",
    "\n",
    "    pop, logbook = algorithms.eaSimple(pop,\n",
    "                               toolbox,\n",
    "                               cxpb=crossover_rate,\n",
    "                               mutpb=mutation_rate,\n",
    "                               ngen=number_of_generations,\n",
    "                               stats=stats,\n",
    "                               halloffame=best_ind,\n",
    "                               verbose=False)\n",
    "\n",
    "    return pop, logbook, best_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_individual_with_probs(individual, test=False):\n",
    "    \n",
    "    global population_with_predictions\n",
    "\n",
    "    if test:\n",
    "        X, y = x_test,y_test\n",
    "    else:\n",
    "        X, y = x_val,y_val\n",
    "\n",
    "    prediction_results = []\n",
    "    for decision_tree_index in individual:\n",
    "        prediction_results.append(list_of_decision_trees[decision_tree_index].predict_proba(X))\n",
    "\n",
    "    prediction_results = np.array(prediction_results)\n",
    "    predictions_of_individual = np.sum(prediction_results, axis=0) / len(individual)\n",
    "\n",
    "    return predictions_of_individual\n",
    "\n",
    "\n",
    "def calculate_fpr(y_preds,y_true):\n",
    "    cnf_matrix = confusion_matrix(y_true, y_preds)\n",
    "    FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix)\n",
    "    FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
    "    TP = np.diag(cnf_matrix)\n",
    "\n",
    "    TN = cnf_matrix.sum() - (FP + FN + TP)\n",
    "    FP = FP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "    return np.mean(FP/(FP+TN))\n",
    "\n",
    "def calculate_precision_recall_auc(y_preds,y_test):\n",
    "    precision_array = []\n",
    "    for i in np.unique(y_test):\n",
    "        one_vs_all_for_specific_class_preds = np.where(y_preds==i,1,0)\n",
    "        one_vs_all_for_specific_class_test = np.where(y_test==i,1,0)\n",
    "        precision_array.append(average_precision_score(one_vs_all_for_specific_class_test, one_vs_all_for_specific_class_preds))\n",
    "    return np.mean(precision_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HGARF_minimize_function(params):\n",
    "    total_number_of_trees, crossover_rate ,mutation_rate = [val for key,val in params.items()]\n",
    "    pop, logbook, best_ind = HGARF(total_number_of_trees=int(total_number_of_trees),\n",
    "                            num_of_features=x_training.shape[1],\n",
    "                            chromosome_length=8,\n",
    "                            number_of_generations=3,\n",
    "                            tour_size=4,\n",
    "                            crossover_rate=crossover_rate,\n",
    "                            mutation_rate=mutation_rate)\n",
    "    val_acc_hgarf = evaluate_individual(best_ind[0])[0]\n",
    "    return 1-val_acc_hgarf\n",
    "\n",
    "def Ada_minimize_function(params):\n",
    "    learning_rate, n_estimators = [val for key,val in params.items()]\n",
    "    clf = AdaBoostClassifier(n_estimators=int(n_estimators), learning_rate=learning_rate)\n",
    "    score = cross_val_score(clf, x_train , y_train, cv=val_folds)\n",
    "    return 1 - (sum(score)/len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_results(params, counter, opti_res):\n",
    "    for idx, row in enumerate(trials):\n",
    "        if counter == 0:\n",
    "            params.append(dict())\n",
    "            params[idx][\"params\"] = row[\"misc\"]['vals']\n",
    "            params[idx]['score'] = 0\n",
    "        params[idx]['score'] += (row['result']['loss']/3)\n",
    "    return params\n",
    "\n",
    "def get_best_params(params):\n",
    "    best_val, best_idx = 100000, -1\n",
    "    for idx, row in enumerate(params):\n",
    "        if row[\"score\"] < best_val:\n",
    "            best_val = row['score']\n",
    "            best_idx = idx\n",
    "    return params[best_idx]['params']\n",
    "\n",
    "\n",
    "def to_one_hot(yi):\n",
    "    b = np.zeros((yi.size, y.max()+1))\n",
    "    b[np.arange(yi.size),yi] = 1\n",
    "    return b\n",
    "\n",
    "\n",
    "def calulate_measure_HGARF(best_params, fold_num, dataset):\n",
    "    row = {}\n",
    "    row[\"Dataset Name\"] = dataset\n",
    "    row[\"Algorithm Name\"] = \"HGARF\"\n",
    "    row[\"Cross Validation\"] = fold_num\n",
    "    num_tress = int(best_params[\"total_number_of_trees\"][0])\n",
    "    num_feat = x_training.shape[1]\n",
    "    cross = best_params[\"crossover_rate\"][0]\n",
    "    mut = best_params[\"mutation_rate\"][0]\n",
    "    start = time.time()\n",
    "    pop, logbook, best_ind = HGARF(total_number_of_trees=num_tress,\n",
    "                            num_of_features=num_feat,\n",
    "                            chromosome_length=8,\n",
    "                            number_of_generations=3,\n",
    "                            tour_size=4,\n",
    "                            crossover_rate=cross,\n",
    "                            mutation_rate=mut)\n",
    "    time_training_hgarf = time.time() - start\n",
    "    row[\"Training Time\"] = time_training_hgarf\n",
    "    start = time.time()\n",
    "    test_acc_hgarf = evaluate_individual(best_ind[0], test=True)[0]\n",
    "    time_inference_hgarf = time.time() - start\n",
    "    row[\"Accuracy\"] = test_acc_hgarf\n",
    "    row[\"Inference Time\"] = time_inference_hgarf\n",
    "    y_preds_hgarf = evaluate_individual_with_probs(best_ind[0], test=True)\n",
    "    row[\"Hyper-Parameters Values\"] = f'{num_tress},{num_feat},8, 3, 4,{cross},{mut}' \n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        #BINARY\n",
    "        roc_auc_hgarf = roc_auc_score(y_test, y_preds_hgarf[:,1],multi_class=\"ovo\")\n",
    "    else:\n",
    "        roc_auc_hgarf = roc_auc_score(y_test, y_preds_hgarf,multi_class=\"ovo\")\n",
    "    \n",
    "    row[\"AUC\"] = roc_auc_hgarf\n",
    "    \n",
    "    hgarf_preds_classes = np.argmax(evaluate_individual_with_probs(best_ind[0], test=True),axis=1)\n",
    "\n",
    "    precision_hgarf = precision_score(hgarf_preds_classes,y_test,average='weighted')\n",
    "    row[\"Precision\"] = precision_hgarf\n",
    "\n",
    "    trp_hgarf = recall_score(hgarf_preds_classes, y_test, average='weighted')\n",
    "    row[\"TPR\"] = trp_hgarf\n",
    "\n",
    "    fpr_hgarf = calculate_fpr(y_test,hgarf_preds_classes)\n",
    "    row[\"FPR\"] = fpr_hgarf\n",
    "\n",
    "    pr_auc_hgarf = calculate_precision_recall_auc(hgarf_preds_classes,y_test)\n",
    "    row[\"PR-Curve\"] = pr_auc_hgarf\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "def calulate_measure_ada(best_params, fold_num, dataset):\n",
    "    row = {}\n",
    "    row[\"Dataset Name\"] = dataset\n",
    "    row[\"Algorithm Name\"] = \"AdaBoost\"\n",
    "    row[\"Cross Validation\"] = fold_num\n",
    "    learning_rate, n_estimators = [val for key,val in best_params.items()]\n",
    "    clf = AdaBoostClassifier(n_estimators=int(n_estimators), learning_rate=learning_rate)\n",
    "    start = time.time()\n",
    "    clf.fit(x_train, y_train)\n",
    "    time_training_ada = time.time() - start\n",
    "    row[\"Training Time\"] = time_training_ada\n",
    "    start = time.time()\n",
    "    test_acc_ada = clf.score(x_test, y_test)\n",
    "    time_inference_ada = time.time() - start\n",
    "    row[\"Accuracy\"] = test_acc_ada\n",
    "    row[\"Inference Time\"] = time_inference_ada\n",
    "    y_preds_adaboost = clf.predict_proba(x_test)\n",
    "    row[\"Hyper-Parameters Values\"] = f'{n_estimators},{learning_rate}' \n",
    "    if len(np.unique(y_test)) == 2:\n",
    "        #BINARY\n",
    "        roc_auc_adaboost = roc_auc_score(y_test, y_preds_adaboost[:,1],multi_class=\"ovo\")\n",
    "    else:\n",
    "        roc_auc_adaboost = roc_auc_score(y_test, y_preds_adaboost,multi_class=\"ovo\")\n",
    "    \n",
    "    row[\"AUC\"] = roc_auc_adaboost\n",
    "    \n",
    "    adaboost_preds_classes = clf.predict(x_test)\n",
    "\n",
    "    precision_adaboost = precision_score(adaboost_preds_classes,y_test,average='weighted')\n",
    "    row[\"Precision\"] = precision_adaboost\n",
    "\n",
    "    trp_adaboost = recall_score(adaboost_preds_classes, y_test, average='weighted')\n",
    "    row[\"TPR\"] = trp_adaboost\n",
    "\n",
    "    fpr_adaboost = calculate_fpr(y_test,adaboost_preds_classes)\n",
    "    row[\"FPR\"] = fpr_adaboost\n",
    "\n",
    "    pr_auc_adaboost = calculate_precision_recall_auc(adaboost_preds_classes,y_test)\n",
    "    row[\"PR-Curve\"] = pr_auc_adaboost\n",
    "    \n",
    "    return row\n",
    "\n",
    "\n",
    "def get_spaces():\n",
    "    HGARF_space = {\"total_number_of_trees\": hp.randint('total_number_of_trees', 50, 100),\n",
    "               \"crossover_rate\": hp.uniform('crossover_rate', 0, 1),\n",
    "               \"mutation_rate\": hp.uniform('mutation_rate', 0, 1)}\n",
    "\n",
    "    ada_space = {\"n_estimators\":  hp.randint('n_estimators', 100, 500),\n",
    "                 'learning_rate': hp.uniform('learning_rate', 0.5, 1.5)}\n",
    "    \n",
    "    return HGARF_space, ada_space\n",
    "\n",
    "\n",
    "def get_running_properties():\n",
    "    test_folds,val_folds, dataset_limit, iter_num = 5, 3, 50, 50\n",
    "    return test_folds,val_folds, dataset_limit, iter_num\n",
    "\n",
    "\n",
    "def get_result_df():\n",
    "    columns = [\"Dataset Name\", \"Algorithm Name\", \"Cross Validation\", \"Hyper-Parameters Values\", \"Accuracy\", \"TPR\",\n",
    "          \"FPR\",\"Precision\", \"AUC\", \"PR-Curve\", \"Training Time\", \"Inference Time\"]\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reach_dataset_limit(idx, dataset_limit):\n",
    "    return idx == dataset_limit\n",
    "\n",
    "\n",
    "def check_for_labels(x_train, y_train, y_test):\n",
    "    if len(np.unique(y_test)) == len(np.unique(y_train)):\n",
    "        return x_train, y_train\n",
    "    else:\n",
    "        missing_values = list(set(np.unique(y_train)) - set(np.unique(y_test)))\n",
    "        to_remove = [i for (i,v) in enumerate(y_train) if v in missing_values] \n",
    "        x_train, y_train = np.delete(x_train,to_remove,0), np.delete(y_train,to_remove) \n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_mail(idx):\n",
    "    port = 465  # For SSL\n",
    "    smtp_server = \"smtp.gmail.com\"\n",
    "    sender_email = \"yarden.experiments@gmail.com\"  # Enter your address\n",
    "    receiver_email = \"rotemyar@post.bgu.ac.il\"  # Enter receiver address\n",
    "    password = 'Nadav123!'\n",
    "    message = f\"Subject: Ensemble project: currently in {idx}/50 datasets.\"\n",
    "\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SMTPDataError",
     "evalue": "(421, b'4.3.0 Temporary System Problem.  Try again later (10). n20sm279041vsr.6 - gsmtp')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSMTPDataError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-2b299fb6e3cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0msend_mail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtime_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total time: {time_end}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ee713a164a89>\u001b[0m in \u001b[0;36msend_mail\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msmtplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSMTP_SSL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmtp_server\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msender_email\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpassword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendmail\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msender_email\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreceiver_email\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/smtplib.py\u001b[0m in \u001b[0;36msendmail\u001b[0;34m(self, from_addr, to_addrs, msg, mail_options, rcpt_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mSMTPRecipientsRefused\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msenderrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 882\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    883\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m421\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/smtplib.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, msg)\u001b[0m\n\u001b[1;32m    558\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m354\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mSMTPDataError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSMTPDataError\u001b[0m: (421, b'4.3.0 Temporary System Problem.  Try again later (10). n20sm279041vsr.6 - gsmtp')"
     ]
    }
   ],
   "source": [
    "toolbox = base.Toolbox()\n",
    "stats = tools.Statistics(key=lambda ind: ind.fitness.values)\n",
    "\n",
    "HGARF_space, ada_space = get_spaces()\n",
    "test_folds,val_folds, dataset_limit, iter_num = get_running_properties()\n",
    "df = get_result_df()\n",
    "start = time.time()\n",
    "for idx, dataset in enumerate(os.listdir(datasets_path)):\n",
    "    if reach_dataset_limit(idx, dataset_limit):\n",
    "        break\n",
    "    if idx % 5 == 0:\n",
    "        send_mail(idx)\n",
    "    time_end = time.time() - start\n",
    "    print(f\"Total time: {time_end}\")\n",
    "    start = time.time()\n",
    "    # Read the current dataset\n",
    "    curr_dataset_path = f'{datasets_path}{dataset}'\n",
    "    X, y, num_of_features = initialize_x_y(curr_dataset_path)\n",
    "\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    stats = tools.Statistics(key=lambda ind: ind.fitness.values)\n",
    "    \n",
    "    # Clearing the .csv at dataset name\n",
    "    dataset = dataset[:-4]\n",
    "    \n",
    "    ## The outer loop - 10 fold CV for train-test split.\n",
    "    skf_test = StratifiedKFold(n_splits=test_folds, random_state=0)\n",
    "    for fold_num, (train_index, test_index) in enumerate(skf_test.split(X, y)):\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Removing the rows where label in each train isn't in test\n",
    "        x_train, y_train = check_for_labels(x_train, y_train, y_test)\n",
    "        \n",
    "        ## The inner loop - the 3 CV (can't use cross_val_score, so we use onlu for HGARF)\n",
    "        skf_val = StratifiedKFold(n_splits=val_folds, random_state=0)\n",
    "        \n",
    "        # A list for each outer fold to save in memory the parameters and there score.\n",
    "        params = []\n",
    "        \n",
    "        ## HGARF part\n",
    "        for counter,(training_index, val_index) in enumerate(skf_val.split(x_train, y_train)):\n",
    "            x_training, x_val = x_train[training_index], x_train[val_index]\n",
    "            y_training, y_val = y_train[training_index], y_train[val_index]\n",
    "            \n",
    "            # Optimize the problem on this inner fold\n",
    "            trials = Trials()\n",
    "            best  = fmin(HGARF_minimize_function,\n",
    "                        space=HGARF_space,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=iter_num,\n",
    "                        trials=trials)  \n",
    "            # Add current fold result to param list\n",
    "            params = cv_results(params, counter, trials)\n",
    "         \n",
    "        x_training, y_training = x_train, y_train\n",
    "        \n",
    "        # Gives us the best params from the total 50 experiments * 3 folds\n",
    "        best_params = get_best_params(params)\n",
    "        \n",
    "        # Calculate all the measurments for this fold and algorithm\n",
    "        row = calulate_measure_HGARF(best_params, fold_num, dataset)\n",
    "        \n",
    "        # insert to final df table\n",
    "        df = df.append(row, ignore_index=True)\n",
    "        \n",
    "        \n",
    "        ## Adaboost part (no need for 3 folds cause we use cross val score)\n",
    "        print(\"Ada\")\n",
    "        trials = Trials()\n",
    "        best_params  = fmin(Ada_minimize_function,\n",
    "                    space=ada_space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=iter_num,\n",
    "                    trials=trials)\n",
    "\n",
    "        # Same as before\n",
    "        row = calulate_measure_ada(best_params, fold_num, dataset)\n",
    "        df = df.append(row, ignore_index=True)\n",
    "        print(df.head())\n",
    "        df.to_csv(f\"./results_{idx}_{fold_num}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset Name</th>\n",
       "      <th>Algorithm Name</th>\n",
       "      <th>Cross Validation</th>\n",
       "      <th>Hyper-Parameters Values</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>TPR</th>\n",
       "      <th>FPR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>AUC</th>\n",
       "      <th>PR-Curve</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Inference Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Dataset Name, Algorithm Name, Cross Validation, Hyper-Parameters Values, Accuracy, TPR, FPR, Precision, AUC, PR-Curve, Training Time, Inference Time]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Temp!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#     start = time.time()\n",
    "#     pop, logbook, best_ind = HGARF(total_number_of_trees=1000,\n",
    "#                             num_of_features=num_of_features,\n",
    "#                             chromosome_length=12,\n",
    "#                             number_of_generations=10,\n",
    "#                             tour_size=10,\n",
    "#                             crossover_rate=0.5,\n",
    "#                             mutation_rate=0.8)\n",
    "#     time_training_hgarf = time.time() - start\n",
    "\n",
    "#     start = time.time()\n",
    "#     clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     time_training_ada_boost = time.time() - start\n",
    "\n",
    "#     start = time.time()\n",
    "#     test_acc_ada_boost = clf.score(x_test, y_test)\n",
    "#     time_inference_ada_boost = time.time() - start    \n",
    "\n",
    "#     start = time.time()\n",
    "#     test_acc_hgarf = evaluate_individual(best_ind[0], test=True)[0]\n",
    "#     time_inference_hgarf = time.time() - start\n",
    "\n",
    "#     y_preds_hgarf = evaluate_individual_with_probs(best_ind[0], test=True)\n",
    "#     y_preds_adaboost = clf.predict_proba(x_test)\n",
    "\n",
    "#     if len(y_test.unique()) == 2:\n",
    "#         #BINARY\n",
    "#         roc_auc_hgarf = roc_auc_score(y_test, y_preds_hgarf[:,1],multi_class=\"ovo\")\n",
    "#         roc_auc_adaboost = roc_auc_score(y_test, y_preds_adaboost[:,1],multi_class=\"ovo\")\n",
    "#     else:\n",
    "#         roc_auc_hgarf = roc_auc_score(y_test, y_preds_hgarf,multi_class=\"ovo\")\n",
    "#         roc_auc_adaboost = roc_auc_score(y_test, y_preds_adaboost,multi_class=\"ovo\")\n",
    "\n",
    "#     hgarf_preds_classes = np.argmax(evaluate_individual_with_probs(best_ind[0], test=True),axis=1)\n",
    "#     adaboost_preds_classes = clf.predict(x_test)\n",
    "\n",
    "#     precision_hgarf = precision_score(hgarf_preds_classes,y_test,average='weighted')\n",
    "#     precision_adaboost = precision_score(adaboost_preds_classes,y_test,average='weighted')\n",
    "\n",
    "#     trp_hgarf = recall_score(hgarf_preds_classes, y_test, average='weighted')\n",
    "#     trp_adaboost = recall_score(adaboost_preds_classes, y_test, average='weighted')\n",
    "\n",
    "#     fpr_hgarf = calculate_fpr(y_test,hgarf_preds_classes)\n",
    "#     fpr_adaboost = calculate_fpr(y_test,adaboost_preds_classes)\n",
    "\n",
    "#     pr_auc_hgarf = calculate_precision_recall_auc(hgarf_preds_classes,y_test)\n",
    "#     pr_auc_adaboost = calculate_precision_recall_auc(adaboost_preds_classes,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HGARF_params = {\"total_number_of_trees\": (100, 2000), \"crossover_rate\": (0,1), \"mutation_rate\": (0,1)}\n",
    "# ada_params = {\"n_estimators\": (100,2000), 'learning_rate': }\n",
    "# test_folds,val_folds = 10, 3\n",
    "# NUM_TRIALS = 50\n",
    "# for i in range(NUM_TRIALS):\n",
    "#     skf_test = StratifiedKFold(n_splits=test_folds, random_state=i)\n",
    "#     for train_index, test_index in skf_test.split(X, y):\n",
    "#         X_train, X_test = X[train_index], X[test_index]\n",
    "#         y_train, y_test = y[train_index], y[test_index]\n",
    "#         skf_val = StratifiedKFold(n_splits=val_folds, random_state=i)\n",
    "#         for j in range(2):\n",
    "#             params = []\n",
    "#             counter = -1\n",
    "#             for training_index, val_index in skf_val.split(X_train, y_train):\n",
    "#                 counter += 1\n",
    "#                 x_training, x_val = X_train[training_index], X_train[val_index]\n",
    "#                 y_training, y_val = y_train[training_index], y_train[val_index]\n",
    "#                 if j==0:\n",
    "#                     HGARF_params[\"num_of_features\"] = (x_training.shape[1],x_training.shape[1])\n",
    "\n",
    "#                     optimizer = BayesianOptimization(\n",
    "#                                 f=HGARF_maximize_function,\n",
    "#                                 pbounds=HGARF_params,\n",
    "#                                 random_state=i)\n",
    "\n",
    "#                     optimizer.maximize(init_points=2, n_iter=5)\n",
    "#                     for i, res in enumerate(optimizer.res):\n",
    "#                         if counter == 0:\n",
    "#                             params[res[\"params\"]] = 0\n",
    "#                         params[res[\"params\"]] += (res[\"target\"] / val_folds)\n",
    "#                     print(params)\n",
    "# #                         print(\"Iteration {}: \\n\\t{}\".format(i, res))\n",
    "#                 else:\n",
    "\n",
    "#                     AdaBoostClassifier(\n",
    "#                     DecisionTreeClassifier(max_depth=int(X_train.shape[1]/2)),\n",
    "#                     n_estimators=1000,\n",
    "#                     learning_rate=1)\n",
    "#             bests_params.append(optimizer.max['params'])        \n",
    "            \n",
    "#             break\n",
    "#         break\n",
    "#     break\n",
    "    \n",
    "# print(optimizer.max['params'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
